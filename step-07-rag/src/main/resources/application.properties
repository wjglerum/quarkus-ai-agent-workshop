# Enable OpenAI with your OPENAI_API_KEY
#quarkus.langchain4j.chat-model.provider=openai
#quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY:}
#quarkus.langchain4j.openai.chat-model.model-name=gpt-4o
#quarkus.langchain4j.openai.chat-model.temperature=1
#quarkus.langchain4j.openai.chat-model.chat-model.max-completion-tokens=1000

# Enable Ollama with OpenAI compatibility
#quarkus.langchain4j.chat-model.provider=openai
#quarkus.langchain4j.openai.base-url=http://localhost:11434/v1/
#quarkus.langchain4j.openai.chat-model.model-name=llama3.2
#quarkus.langchain4j.openai.chat-model.temperature=1
#quarkus.langchain4j.openai.chat-model.chat-model.max-completion-tokens=1000

# Enable Google Gemini with your GEMINI_API_KEY
#quarkus.langchain4j.chat-model.provider=gemini
#quarkus.langchain4j.ai.gemini.api-key=${GEMINI_API_KEY}
#quarkus.langchain4j.ai.gemini.chat-model.model-id=gemini-2.5-flash

# Enable Ollama
#quarkus.langchain4j.chat-model.provider=ollama
#quarkus.langchain4j.ollama.chat-model.model-id=llama3.2

# Capture requests and response from the language model
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true

# Set larger timeout for local language models
quarkus.langchain4j.timeout=1m

# Configure temperate globally for all LLMs
quarkus.langchain4j.temperature=1

# Enable OIDC Dev Services
quarkus.oidc.devservices.enabled=false
quarkus.oidc.application-type=hybrid

# Enforce authentication for all endpoints
quarkus.http.auth.permission.authenticated.paths=/*
quarkus.http.auth.permission.authenticated.policy=authenticated

# Log REST client request-response
quarkus.rest-client.logging.scope=request-response

# Set Weather MCP client
quarkus.langchain4j.mcp.weather.transport-type=http
quarkus.langchain4j.mcp.weather.url=http://localhost:8081/mcp/sse

# EasyRAG watches this folder and ingests on startup
quarkus.langchain4j.easy-rag.path=src/main/resources/rag
quarkus.langchain4j.easy-rag.max-segment-size=100
quarkus.langchain4j.easy-rag.max-overlap-size=25
quarkus.langchain4j.easy-rag.max-results=30

# Use the local ONNX embedding model
quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.onnx.bgesmallenq.BgeSmallEnQuantizedEmbeddingModel